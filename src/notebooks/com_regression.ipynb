{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd4b29cc-073a-4a70-a003-10d334421bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3880e6a3-aec5-46d9-9872-0437a2b28495",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./runs/classification_all/')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0577456a-c917-40f5-8e72-b57a8924dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COMDataset(Dataset):\n",
    "    def __init__(self, production_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.prodiction_dir = production_dir\n",
    "        self.com_dir = os.path.join(production_dir, 'com_dir')\n",
    "        self.com_list = []\n",
    "        self.com_files = os.listdir(self.com_dir)\n",
    "        self.com_files = [f for f in os.listdir(self.com_dir) if f.endswith('.txt')]\n",
    "        self.com_files.sort(key=self.sort_coms)\n",
    "        for window, file in enumerate(self.com_files):\n",
    "            self.com_list.append(pd.read_csv(os.path.join(self.com_dir, file), header=None, names=[window], usecols=[0]))\n",
    "        self.com_list = np.array(self.com_list)\n",
    "        for idxs in range(len(self.com_list)):\n",
    "            for i in range(len(self.com_list[idxs])):\n",
    "                self.com_list[idxs][i][0] = np.float64(self.com_list[idxs][i][0][9:].strip()) \n",
    "        self.com_list = self.com_list.squeeze()\n",
    "        \n",
    "        self.n_windows = len(self.com_files)\n",
    "        self.window_paths = [os.path.join(self.prodiction_dir, str(window), 'hb_observable.txt') for window in range(self.n_windows)]\n",
    "        self.hb_list = []\n",
    "        for window, path in enumerate(self.window_paths):\n",
    "            self.hb_list.append(pd.read_csv(path, header=None,names=[window], usecols=[0]))        \n",
    "        self.hb_list = np.array(self.hb_list).squeeze()\n",
    "        \n",
    "        self.x = np.reshape(self.com_list, self.com_list.shape[0] * self.com_list.shape[1])\n",
    "        self.y = np.reshape(self.hb_list, self.hb_list.shape[0] * self.hb_list.shape[1])\n",
    "\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        \n",
    "    def sort_coms(self, file):\n",
    "        # Method to sort com files by window number\n",
    "        var = int(file.split('_')[-1].split('.')[0])\n",
    "        return var\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index], self.y[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a77217f6-2234-4a0e-91eb-b987ef78beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_dir = '/scratch/mlsample/ipy_oxDNA/ipy_oxdna_examples/duplex_melting/us_melting/production/'\n",
    "dataset  = COMDataset(production_dir)\n",
    "batch_size = 4096\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b23a9a5c-6ee8-4a48-ac20-f6b9355d7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c4c3d5f-1dd2-46a9-9bef-b55d1141fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "input_size = 1 # 28x28\n",
    "hidden_size = 640\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0284fa78-104e-419c-8f79-09f797fc3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6170cd7-c559-484d-8368-cbe17d603627",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10 - Loss: 0.3058 Acc: 0.8863\n",
      "Epoch 0/10 - Loss: 0.3047 Acc: 0.8817\n",
      "Epoch 0/10 - Loss: 0.3043 Acc: 0.8798\n",
      "Epoch 0/10 - Loss: 0.3041 Acc: 0.8791\n",
      "Epoch 0/10 - Loss: 0.3042 Acc: 0.8786\n",
      "Epoch 0/10 - Loss: 0.3039 Acc: 0.8783\n",
      "Epoch 0/10 - Loss: 0.3040 Acc: 0.8780\n",
      "Epoch 0/10 - Loss: 0.3041 Acc: 0.8778\n",
      "Epoch 0/10 - Loss: 0.3040 Acc: 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:12<01:55, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.3047 Acc: 0.8863\n",
      "Epoch 1/10 - Loss: 0.3037 Acc: 0.8818\n",
      "Epoch 1/10 - Loss: 0.3039 Acc: 0.8799\n",
      "Epoch 1/10 - Loss: 0.3041 Acc: 0.8791\n",
      "Epoch 1/10 - Loss: 0.3038 Acc: 0.8787\n",
      "Epoch 1/10 - Loss: 0.3037 Acc: 0.8784\n",
      "Epoch 1/10 - Loss: 0.3038 Acc: 0.8781\n",
      "Epoch 1/10 - Loss: 0.3036 Acc: 0.8780\n",
      "Epoch 1/10 - Loss: 0.3037 Acc: 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:25<01:42, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 0.3078 Acc: 0.8852\n",
      "Epoch 2/10 - Loss: 0.3053 Acc: 0.8810\n",
      "Epoch 2/10 - Loss: 0.3046 Acc: 0.8795\n",
      "Epoch 2/10 - Loss: 0.3045 Acc: 0.8787\n",
      "Epoch 2/10 - Loss: 0.3041 Acc: 0.8784\n",
      "Epoch 2/10 - Loss: 0.3042 Acc: 0.8780\n",
      "Epoch 2/10 - Loss: 0.3040 Acc: 0.8778\n",
      "Epoch 2/10 - Loss: 0.3035 Acc: 0.8779\n",
      "Epoch 2/10 - Loss: 0.3034 Acc: 0.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:37<01:26, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 0.3051 Acc: 0.8859\n",
      "Epoch 3/10 - Loss: 0.3037 Acc: 0.8815\n",
      "Epoch 3/10 - Loss: 0.3036 Acc: 0.8799\n",
      "Epoch 3/10 - Loss: 0.3036 Acc: 0.8790\n",
      "Epoch 3/10 - Loss: 0.3034 Acc: 0.8785\n",
      "Epoch 3/10 - Loss: 0.3032 Acc: 0.8784\n",
      "Epoch 3/10 - Loss: 0.3033 Acc: 0.8780\n",
      "Epoch 3/10 - Loss: 0.3033 Acc: 0.8779\n",
      "Epoch 3/10 - Loss: 0.3032 Acc: 0.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:50<01:16, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 0.3062 Acc: 0.8853\n",
      "Epoch 4/10 - Loss: 0.3054 Acc: 0.8808\n",
      "Epoch 4/10 - Loss: 0.3045 Acc: 0.8796\n",
      "Epoch 4/10 - Loss: 0.3037 Acc: 0.8790\n",
      "Epoch 4/10 - Loss: 0.3038 Acc: 0.8785\n",
      "Epoch 4/10 - Loss: 0.3034 Acc: 0.8783\n",
      "Epoch 4/10 - Loss: 0.3033 Acc: 0.8780\n",
      "Epoch 4/10 - Loss: 0.3033 Acc: 0.8779\n",
      "Epoch 4/10 - Loss: 0.3032 Acc: 0.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:03<01:03, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Loss: 0.3054 Acc: 0.8856\n",
      "Epoch 5/10 - Loss: 0.3037 Acc: 0.8813\n",
      "Epoch 5/10 - Loss: 0.3025 Acc: 0.8801\n",
      "Epoch 5/10 - Loss: 0.3028 Acc: 0.8792\n",
      "Epoch 5/10 - Loss: 0.3029 Acc: 0.8788\n",
      "Epoch 5/10 - Loss: 0.3029 Acc: 0.8784\n",
      "Epoch 5/10 - Loss: 0.3027 Acc: 0.8782\n",
      "Epoch 5/10 - Loss: 0.3028 Acc: 0.8780\n",
      "Epoch 5/10 - Loss: 0.3029 Acc: 0.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:15<00:50, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Loss: 0.3061 Acc: 0.8855\n",
      "Epoch 6/10 - Loss: 0.3042 Acc: 0.8812\n",
      "Epoch 6/10 - Loss: 0.3041 Acc: 0.8794\n",
      "Epoch 6/10 - Loss: 0.3036 Acc: 0.8789\n",
      "Epoch 6/10 - Loss: 0.3035 Acc: 0.8784\n",
      "Epoch 6/10 - Loss: 0.3031 Acc: 0.8783\n",
      "Epoch 6/10 - Loss: 0.3031 Acc: 0.8780\n",
      "Epoch 6/10 - Loss: 0.3031 Acc: 0.8778\n",
      "Epoch 6/10 - Loss: 0.3028 Acc: 0.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:28<00:37, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Loss: 0.3064 Acc: 0.8853\n",
      "Epoch 7/10 - Loss: 0.3046 Acc: 0.8811\n",
      "Epoch 7/10 - Loss: 0.3038 Acc: 0.8797\n",
      "Epoch 7/10 - Loss: 0.3031 Acc: 0.8792\n",
      "Epoch 7/10 - Loss: 0.3029 Acc: 0.8788\n",
      "Epoch 7/10 - Loss: 0.3027 Acc: 0.8786\n",
      "Epoch 7/10 - Loss: 0.3027 Acc: 0.8783\n",
      "Epoch 7/10 - Loss: 0.3026 Acc: 0.8780\n",
      "Epoch 7/10 - Loss: 0.3027 Acc: 0.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:40<00:24, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 0.3052 Acc: 0.8856\n",
      "Epoch 8/10 - Loss: 0.3032 Acc: 0.8815\n",
      "Epoch 8/10 - Loss: 0.3032 Acc: 0.8799\n",
      "Epoch 8/10 - Loss: 0.3028 Acc: 0.8791\n",
      "Epoch 8/10 - Loss: 0.3026 Acc: 0.8788\n",
      "Epoch 8/10 - Loss: 0.3025 Acc: 0.8785\n",
      "Epoch 8/10 - Loss: 0.3026 Acc: 0.8783\n",
      "Epoch 8/10 - Loss: 0.3025 Acc: 0.8781\n",
      "Epoch 8/10 - Loss: 0.3026 Acc: 0.8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:53<00:12, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Loss: 0.3051 Acc: 0.8856\n",
      "Epoch 9/10 - Loss: 0.3040 Acc: 0.8813\n",
      "Epoch 9/10 - Loss: 0.3030 Acc: 0.8800\n",
      "Epoch 9/10 - Loss: 0.3026 Acc: 0.8793\n",
      "Epoch 9/10 - Loss: 0.3027 Acc: 0.8787\n",
      "Epoch 9/10 - Loss: 0.3027 Acc: 0.8785\n",
      "Epoch 9/10 - Loss: 0.3026 Acc: 0.8782\n",
      "Epoch 9/10 - Loss: 0.3025 Acc: 0.8781\n",
      "Epoch 9/10 - Loss: 0.3025 Acc: 0.8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:05<00:00, 12.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.float().view(-1, input_size).to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == y.data)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            epoch_loss = running_loss / (i * batch_size)\n",
    "            epoch_acc = running_corrects.double() / (i * batch_size)\n",
    "            print(f'Epoch {epoch}/{num_epochs} - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            writer.add_scalar('training loss', epoch_loss, epoch * n_total_steps + i)\n",
    "            writer.add_scalar('accuracy', epoch_acc, epoch * n_total_steps + i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b22cb1b-b2a0-42f4-9b52-4e23746ab15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1168675/201484662.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.6718255108582\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_sample = 0\n",
    "    for x, y in test_loader:\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = x.view(x.shape[0], 1).to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(x)\n",
    "        #value, index\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_sample += y.shape[0]\n",
    "        n_correct += (predictions == y).sum().item()\n",
    "        \n",
    "acc = 100.0 * n_correct / n_sample\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c502a4-155c-4748-a6e4-108eb94cb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir './runs/classification_all/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
